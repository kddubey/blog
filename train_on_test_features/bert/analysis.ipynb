{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**: Tries to answer [my answer on\n",
    "stats.stackexchange.com](https://stats.stackexchange.com/q/611877/337906) by training\n",
    "BERT on real classification datasets. This experiment suggests that training on test set\n",
    "features (no labels) can be okay.\n",
    "\n",
    "**Estimated runtime**: ~30 minutes on a Google Colab GPU or TPU. Eternity on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import permutation_test\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import fdrcorrection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"ag_news\"\n",
    "# dataset = \"enron_spam\"\n",
    "# dataset = \"amazon_counterfactual_en\"\n",
    "# dataset = \"yelp_review_full\"\n",
    "# dataset = \"craigslist_bargains\"\n",
    "# dataset = \"emotion\"\n",
    "# dataset = \"ethos\"\n",
    "# dataset = \"yahoo_answers_topics\"\n",
    "# dataset = \"trec\"\n",
    "# dataset = \"mtop_domain\"\n",
    "# dataset = \"clickbait_notclickbait_dataset\"\n",
    "# dataset = \"financial_phrasebank\"\n",
    "# dataset = \"app_reviews\"\n",
    "dataset = \"rotten_tomatoes\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(\"bert_accuracies\", f\"{dataset}.csv\"))\n",
    "print(df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_test(\n",
    "    data=(df[\"test\"], df[\"extra\"]),\n",
    "    statistic=lambda x, y: np.mean(x - y),\n",
    "    alternative=\"greater\",  # acc_test (unfair) > acc_extra (fair)\n",
    "    permutation_type=\"samples\",  # paired observations\n",
    "    n_resamples=10_000,\n",
    ").pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=len(df.columns), ncols=1, figsize=(6, len(df.columns) * 1.5)\n",
    ")\n",
    "\n",
    "axes: list[plt.Axes]\n",
    "\n",
    "x_common_min = df.min().min()\n",
    "x_common_max = df.max().max()\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "# Plot histograms for each column\n",
    "for i, column in enumerate(df.columns):\n",
    "    column: str\n",
    "    df[column].hist(\n",
    "        bins=bins, ax=axes[i], range=(x_common_min, x_common_max), color=colors[i]\n",
    "    )\n",
    "    axes[i].set_ylim((0, 20))\n",
    "    axes[i].set_xlabel(f\"{column} accuracy\")\n",
    "    axes[i].set_ylabel(\"frequency\")\n",
    "\n",
    "fig.suptitle(f\"Dataset: {dataset}\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = [\n",
    "    0.65,\n",
    "    0.63,\n",
    "    0.58,\n",
    "    0.09,\n",
    "    0.84,\n",
    "    0.78,\n",
    "    0.18,\n",
    "    0.9988,\n",
    "    0.28,\n",
    "    0.86,\n",
    "    0.86,\n",
    "    0.29,\n",
    "    0.09,\n",
    "    0.71,\n",
    "]\n",
    "fdrcorrection(pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dfs = []\n",
    "for accuracy_csv in sorted(os.listdir(\"bert_accuracies\")):\n",
    "    _df = pd.read_csv(os.path.join(\"bert_accuracies\", accuracy_csv))\n",
    "    _df[\"dataset\"] = accuracy_csv.removesuffix(\".csv\")\n",
    "    _dfs.append(_df)\n",
    "accuracy_df = pd.concat(_dfs)\n",
    "accuracy_df = accuracy_df[[\"dataset\", \"base\", \"extra\", \"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 200  # taken from bert/run.ipynb\n",
    "num_correct_df = (accuracy_df.copy()[[\"base\", \"extra\", \"test\"]] * num_test).astype(int)\n",
    "num_correct_df[\"dataset\"] = accuracy_df[\"dataset\"].copy()\n",
    "num_correct_df = num_correct_df[[\"dataset\", \"base\", \"extra\", \"test\"]]\n",
    "num_correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df[\"diff\"] = accuracy_df[\"test\"] - accuracy_df[\"extra\"]\n",
    "accuracy_df[\"control\"] = accuracy_df[\"extra\"] - accuracy_df[\"base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does pretraining help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(accuracy_df\n",
    " .groupby(\"dataset\")\n",
    " [\"control\"]\n",
    " .describe()\n",
    " [[\"mean\", \"std\"]]\n",
    " .round(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 2))\n",
    "axes: plt.Axes\n",
    "sns.violinplot(data=accuracy_df, x=\"dataset\", y=\"control\", ax=axes)\n",
    "axes.set_title(\"BERT for text classification\")\n",
    "\n",
    "axes.yaxis.grid(True)\n",
    "axes.set_xlabel(\"Dataset\")\n",
    "axes.set_ylabel(\n",
    "    \"Accuracy boost\\n\"\n",
    "    \"($\\\\text{acc}_\\\\text{extra}$ - $\\\\text{acc}_\\\\text{base}$)\",\n",
    "    rotation=\"horizontal\",\n",
    "    ha=\"right\",\n",
    "    va=\"top\",\n",
    ")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does pretraining on test cause bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(accuracy_df\n",
    " .groupby(\"dataset\")\n",
    " [\"diff\"]\n",
    " .describe()\n",
    " [[\"mean\", \"std\"]]\n",
    " .round(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 2))\n",
    "axes: plt.Axes\n",
    "sns.violinplot(data=accuracy_df, x=\"dataset\", y=\"diff\", ax=axes, color=\"darkorange\")\n",
    "axes.set_title(\"BERT for text classification\")\n",
    "\n",
    "axes.yaxis.grid(True)\n",
    "axes.set_xlabel(\"Dataset\")\n",
    "axes.set_ylabel(\n",
    "    \"Accuracy overestimation\\n\"\n",
    "    \"($\\\\text{acc}_\\\\text{test}$ - $\\\\text{acc}_\\\\text{extra}$)\",\n",
    "    rotation=\"horizontal\",\n",
    "    ha=\"right\",\n",
    "    va=\"center\",\n",
    ")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
