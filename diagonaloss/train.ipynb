{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most code taken from:\n",
    "https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/matryoshka/matryoshka_nli.py\n",
    "\n",
    "TODO: PR to make it produce the plots\n",
    "[here](https://www.sbert.net/examples/training/matryoshka/README.html#results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import gzip\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from typing import Any, Callable\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sentence_transformers import (\n",
    "    models,\n",
    "    losses,\n",
    "    datasets,\n",
    "    LoggingHandler,\n",
    "    SentenceTransformer,\n",
    "    util,\n",
    "    InputExample,\n",
    ")\n",
    "from sentence_transformers.evaluation import (\n",
    "    EmbeddingSimilarityEvaluator,\n",
    "    SimilarityFunction,\n",
    ")\n",
    "from sentence_transformers.util import cos_sim\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=print,\n",
    "    handlers=[LoggingHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = sys.argv[1] if len(sys.argv) > 1 else \"distilroberta-base\"\n",
    "train_batch_size = 128\n",
    "max_seq_length = 75\n",
    "num_epochs = 1\n",
    "\n",
    "MATRYOSHKA_DIMENSIONS = [768, 512, 256, 128, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not yet similarity trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = (\n",
    "    \"output/matryoshka_nli_\"\n",
    "    + model_name.replace(\"/\", \"-\")\n",
    "    + \"-\"\n",
    "    + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    ")\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\"\n",
    ")\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists. If not, download and extract  it\n",
    "nli_dataset_path = \"data/AllNLI.tsv.gz\"\n",
    "\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get(\"https://sbert.net/datasets/AllNLI.tsv.gz\", nli_dataset_path)\n",
    "\n",
    "# Read the AllNLI.tsv.gz file and create the training dataset\n",
    "print(\"Read AllNLI train dataset\")\n",
    "\n",
    "\n",
    "train_data: dict[str, dict[str, set]] = {}\n",
    "# {\n",
    "#     sentence: {\n",
    "#         \"contradiction\": contr sentence,\n",
    "#         \"entailment\": entail sentence,\n",
    "#         \"neutral\": neutral sentence\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "def add_to_samples(sent1, sent2, label):\n",
    "    if sent1 not in train_data:\n",
    "        train_data[sent1] = {\n",
    "            \"contradiction\": set(),\n",
    "            \"entailment\": set(),\n",
    "            \"neutral\": set(),\n",
    "        }\n",
    "    train_data[sent1][label].add(sent2)\n",
    "\n",
    "\n",
    "with gzip.open(nli_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row[\"split\"] == \"train\":\n",
    "            sent1 = row[\"sentence1\"].strip()\n",
    "            sent2 = row[\"sentence2\"].strip()\n",
    "\n",
    "            add_to_samples(sent1, sent2, row[\"label\"])\n",
    "            add_to_samples(sent2, sent1, row[\"label\"])  # Also add the opposite\n",
    "\n",
    "\n",
    "train_samples = []\n",
    "for sent1, others in train_data.items():\n",
    "    if len(others[\"entailment\"]) > 0 and len(others[\"contradiction\"]) > 0:\n",
    "        train_samples.append(\n",
    "            InputExample(\n",
    "                texts=[\n",
    "                    sent1,\n",
    "                    random.choice(list(others[\"entailment\"])),\n",
    "                    random.choice(list(others[\"contradiction\"])),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        train_samples.append(\n",
    "            InputExample(\n",
    "                texts=[\n",
    "                    random.choice(list(others[\"entailment\"])),\n",
    "                    sent1,\n",
    "                    random.choice(list(others[\"contradiction\"])),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(\"Train samples: {}\".format(len(train_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special data loader that avoid duplicates within a batch\n",
    "train_dataloader = datasets.NoDuplicatesDataLoader(\n",
    "    train_samples, batch_size=train_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training\n",
    "warmup_steps = math.ceil(\n",
    "    len(train_dataloader) * num_epochs * 0.1\n",
    ")  # 10% of train data for warm-up\n",
    "print(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "train_loss = losses.MatryoshkaLoss(model, train_loss, MATRYOSHKA_DIMENSIONS)\n",
    "\n",
    "\n",
    "stsb_dev = load_dataset(\"mteb/stsbenchmark-sts\", split=\"validation\")\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    stsb_dev[\"sentence1\"],\n",
    "    stsb_dev[\"sentence2\"],\n",
    "    [score / 5 for score in stsb_dev[\"score\"]],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts-dev\",\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=dev_evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=int(len(train_dataloader) * 0.1),\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    "    use_amp=False,  # Set to True, if your GPU supports FP16 operations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on STS benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stsb_test = load_dataset(\"mteb/stsbenchmark-sts\", split=\"test\")\n",
    "\n",
    "test_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    stsb_test[\"sentence1\"],\n",
    "    stsb_test[\"sentence2\"],\n",
    "    [score / 5 for score in stsb_test[\"score\"]],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts-test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch encode to truncate lol\n",
    "@contextmanager\n",
    "def _monkeypatch_instance_method(obj: Any, method_name: str, new_method: Callable):\n",
    "    original_method = getattr(obj, method_name)\n",
    "    # Need to use __get__ when patching methods\n",
    "    # https://stackoverflow.com/a/28127947/18758987\n",
    "    try:\n",
    "        setattr(obj, method_name, new_method.__get__(obj, obj.__class__))\n",
    "        yield\n",
    "    finally:\n",
    "        setattr(obj, method_name, original_method.__get__(obj, obj.__class__))\n",
    "\n",
    "@contextmanager\n",
    "def _monkeypatch_instance_method(obj: Any, method_name: str, new_method: Callable):\n",
    "    original_method = getattr(obj, method_name)\n",
    "    # Need to use __get__ when patching methods\n",
    "    # https://stackoverflow.com/a/28127947/18758987\n",
    "    try:\n",
    "        setattr(obj, method_name, new_method.__get__(obj, obj.__class__))\n",
    "        yield\n",
    "    finally:\n",
    "        setattr(obj, method_name, original_method.__get__(obj, obj.__class__))\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def truncate_embeddings(model: SentenceTransformer, dim: int):\n",
    "    \"\"\"\n",
    "    In this context, the model outputs embeddings truncated at dimension `dim`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : SentenceTransformer\n",
    "        model where `model.encode` outputs a (D,) or (N, D) array or tensor of\n",
    "        embeddings\n",
    "    dim : int\n",
    "        dimension to truncate at. So a (N, D) array becomes (N, `dim`)\n",
    "    \"\"\"\n",
    "\n",
    "    original_encode = model.encode\n",
    "\n",
    "    @wraps(original_encode)\n",
    "    def encode(self, *args, **kwargs) -> np.ndarray | torch.Tensor:\n",
    "        embeddings = original_encode(*args, **kwargs)\n",
    "        return embeddings[..., :dim]\n",
    "\n",
    "    with _monkeypatch_instance_method(model, \"encode\", encode):\n",
    "        yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in tqdm(MATRYOSHKA_DIMENSIONS, desc=\"Dimensions\"):\n",
    "    output_path = f\"{model_save_path}-dim{dim}\"\n",
    "    with truncate_embeddings(model, dim):\n",
    "        test_evaluator(model, output_path=output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
