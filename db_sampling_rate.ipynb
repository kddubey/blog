{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a database, there lives a table. You want to compute a statistic using that\n",
    "table. Ideally, you'd run your query like so:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    SUM(\"field\") AS statistic\n",
    "FROM ginormous_table\n",
    "WHERE \"condition\";\n",
    "```\n",
    "\n",
    "This query takes a long time because `ginormous_table` is ginormous. So you exploit a\n",
    "feature supported by most databases: sampling. For Postgres DBs, the sampled query looks\n",
    "something like:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    SUM(\"field\") * (100 / p) AS estimate  -- p% of table sampled, whatsupscale\n",
    "FROM ginormous_table TABLESAMPLE BERNOULLI (p)\n",
    "WHERE \"condition\";\n",
    "```\n",
    "\n",
    "The sampled query is significantly faster than the plain one. But anyone who has worked\n",
    "with such queries knows that setting `p`—the percentage of the table to sample—is\n",
    "tricky. Increasing `p` results in better estimates for the statistic, and may be\n",
    "necessary when the `condition` is rarely met. But setting it too high causes the query\n",
    "to take too long. What's the lowest `p` we can get away with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've seen this question pop up twice in work settings. The best answer is to do the hard\n",
    "work of empirically evaluating different sampling percentages on real data. But let's\n",
    "see if we can ballpark `p` given ballpark or worst-case guesses for certain statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some definitions:\n",
    "\n",
    "* The table contains $N$ records (at the time that the query is run).\n",
    "\n",
    "* We'll sample $n \\leq N$ of them to compute the statistic. `p` = $n/N$.\n",
    "\n",
    "* The random variable $X_i \\sim \\text{Bernoulli}(\\lambda) \\text{ i.i.d.}$ for\n",
    "  $i = 1, 2, \\dots, n$ indicates whether or not the $i$'th sampled record of the table\n",
    "  matches the condition. It's safe to assume that they're independent b/c they're\n",
    "  randomly sampled.\n",
    "\n",
    "* The random variable $Y_i$ is the value of the field corresponding to sampled record\n",
    "  $i$. We sum this field when computing the statistic of interest. If you're just trying\n",
    "  to count, then $Y_i$ is not random; it's always $1$.\n",
    "  * Assume they're i.i.d. w/ finite mean and variance Call $\\mu_Y = \\mathbb{E}(Y_i)$ and\n",
    "    $\\sigma^2_Y = \\text{Var}(Y_i)$.\n",
    "  * I'm pretty sure that it's very wrong to assume that $Y_i$'s are non-random in\n",
    "    non-counting problems. There's variation in them through the random sample.\n",
    "\n",
    "* The parameter of interest is what's expressed in the plain SQL query above:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu &= \\sum_{i \\in \\{i \\: | \\: X_i = 1 \\} } Y_i \\\\\n",
    "&= \\sum_{i=1}^{N} X_i Y_i \\\\\n",
    "&= (N \\lambda) \\mu_Y.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "* Assume that $X_i \\perp Y_j$ for all $i, j$. This assumption means that the number of\n",
    "  records matching the condition is not correlated with the value of the field.\n",
    "  * If they're positively correlated (assumption violated), but you don't really care\n",
    "    about poorly estimating $\\mu$ for a rare condition, then you shouldn't worry too\n",
    "    much about setting $n$ too low. In other words, this analysis won't have much value\n",
    "    to you.\n",
    "\n",
    "* Let $M$ be a random variable for the number of records matching the condition *in the\n",
    "  sample*:\n",
    "\n",
    "  $$\n",
    "  \\begin{equation*}\n",
    "  M = \\sum_{i=1}^{n} X_i.\n",
    "  \\end{equation*}\n",
    "  $$\n",
    "\n",
    "  * In case it becomes useful, it's worth noting that in many actual queries, $n$ is\n",
    "    pretty large and $\\lambda$ is pretty small. This means $M$ is approximately\n",
    "    Poisson-distributed, which is mathematically and computationally nicer than the\n",
    "    Binomial distribution. I don't think we'll need this fact anywhere.\n",
    "\n",
    "* The estimator for $\\mu$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\mu} &= \\Bigg( \\sum_{i=1}^{n} X_i Y_i \\Bigg) \\frac{N}{n}\\\\\n",
    "&= \\Bigg( \\sum_{i=1}^{M} Y_i \\Bigg) \\frac{N}{n}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(I'm being sloppy with the subscripts. The $i$'s at this point are just for counting\n",
    "purposes, which will be fine for the math b/c things are identically distributed.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimator is the sum of a random number of random variables. So we'll need the facts\n",
    "about expectation and variance [here](https://sites.stat.washington.edu/thompson/Stat395_09/Notes/week10.pdf).\n",
    "\n",
    "$\\hat{\\mu}$ is unbiased:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(\\hat{\\mu}) &= \\frac{N}{n} \\mathbb{E} \\Bigg( \\sum_{i=1}^{M} Y_i \\Bigg) \\\\\n",
    "&= \\frac{N}{n} \\Bigg( \\sum_{i=1}^{M} \\mathbb{E}(Y_i) \\Bigg) \\\\\n",
    "&= \\frac{N}{n} (n \\lambda) \\mu_Y \\\\\n",
    "&= (N \\lambda) \\mu_Y.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The variance is finite:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sigma^2 &= \\text{Var}(\\hat{\\mu}) \\\\\n",
    "&= \\bigg( \\frac{N}{n} \\bigg)^2 \\text{Var} \\Bigg( \\sum_{i=1}^{M}\n",
    "Y_i \\Bigg) \\\\\n",
    "&= \\bigg( \\frac{N}{n} \\bigg)^2 \\bigg( \\sigma^2_Y n \\lambda + \\mu^2_Y \\big(\n",
    "n\\lambda(1-\\lambda) \\big) \\bigg).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the lowest sample size needed to satisfy some error criterion, we typically\n",
    "need to know the quantiles of the distribution of $\\hat{\\mu}$. I looked it up for the\n",
    "case where the estimator is the sum of a random number of random variables, and [it's\n",
    "not very\n",
    "nice](https://math.stackexchange.com/questions/338129/random-sum-of-random-variables).\n",
    "(If all $Y_i = 1$ b/c you're just counting records in a DB, then $\\hat{\\mu}$ is a\n",
    "rescaled binomial.)\n",
    "\n",
    "So let's just throw a bound at it. I'm thinking\n",
    "[Chebyshev's](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality). There are others,\n",
    "but they're either worse (too high sample sizes) or too complicated for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify Chebyshev's bound in two ways.\n",
    "\n",
    "First, note that Chebyshev's bound is one on absolute error. Relative error seems more\n",
    "interpretable for this application b/c we presumably will run the query over many\n",
    "different conditions w/ many different $\\mu$ values. If $\\mu > 0$ (there are records\n",
    "matching the condition in the table and their values are positive), then we can write a\n",
    "bound on relative error:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Pr(|\\hat{\\mu} - \\mu| \\geq k\\sigma) &\\leq \\frac{1}{k^2} \\\\\n",
    "\\Pr \\bigg( \\frac{|\\hat{\\mu} - \\mu|}{\\mu} \\geq \\frac{k\\sigma}{\\mu} \\bigg) &\\leq\n",
    "\\frac{1}{k^2}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Next, it's easier for a user to input some $\\epsilon > 0$ for the error instead of\n",
    "$k$ standard deviations.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\epsilon &= \\frac{k \\sigma}{\\mu} \\\\\n",
    "k &= \\frac{\\epsilon \\mu}{\\sigma} \\\\\n",
    "\\frac{1}{k^2} &= \\bigg( \\frac{\\sigma}{\\epsilon \\mu} \\bigg)^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So our $\\epsilon$ + relative version of Chebyshev's inequality is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Pr \\bigg( \\frac{|\\hat{\\mu} - \\mu|}{\\mu} \\geq \\epsilon \\bigg) &\\leq\n",
    "\\frac{\\sigma^2}{\\epsilon^2 \\mu^2}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the probability on LHS $p$.\n",
    "\n",
    "To recover the sample size, plug in $\\sigma^2 = \\text{Var}(\\hat{\\mu})$. And we get our\n",
    "sample size calculator:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\epsilon^2 \\mu^2 p &= \\bigg( \\frac{N}{n} \\bigg)^2 \\bigg( \\sigma^2_Y n \\lambda + \\mu^2_Y\n",
    "\\big( n\\lambda(1-\\lambda) \\big) \\bigg) \\\\\n",
    "\\epsilon^2 \\mu^2 p &= \\frac{N^2}{n} \\bigg( \\sigma^2_Y \\lambda + \\mu^2_Y\n",
    "\\big( \\lambda(1-\\lambda) \\big) \\bigg) \\\\\n",
    "n &= \\frac{N^2 \\bigg( \\sigma^2_Y \\lambda + \\mu^2_Y\n",
    "\\big( \\lambda(1-\\lambda) \\big) \\bigg)}{\\epsilon^2 \\mu^2 p}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\epsilon$ = maximum tolerated relative error ($ 0 < \\epsilon < 1$)\n",
    "* $p$ = maximum tolerated probability of the estimator resulting in $\\epsilon$-or-worse\n",
    "  relative error, i.e., if we ran the sampled query $100$ times, then expect the\n",
    "  estimate to have $\\epsilon$-or-worse relative error in at most $100p$ of them \n",
    "* $N$ = number of records in the table (at the time of the query)\n",
    "* $\\lambda$ = fraction of all records matching the condition\n",
    "* $\\sigma^2_Y$ = \"true\" variance of the field (which we're summing over in the query)\n",
    "  for records matching the condition\n",
    "* $\\mu_Y$ = \"true\" mean of the field (which we're summing over in the query)\n",
    "  for records matching the condition.\n",
    "\n",
    "Drop the $\\mu^2_Y$ term from the denominator if $\\epsilon$ is the maximum tolerated\n",
    "*absolute* error, not relative error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all $Y_i = 1$ b/c we're just counting (not summing a non-binary field), then $\\mu_Y =\n",
    "1$ and $\\sigma^2_Y = 0$, so the sample size calculator collapses to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "n &= \\frac{N^2 \\bigg( 0 \\lambda + 1\n",
    "\\big( \\lambda(1-\\lambda) \\big) \\bigg)}{\\epsilon^2 (N\\lambda)^2 p} \\\\\n",
    "&= \\frac{1-\\lambda}{\\epsilon^2 \\lambda p} \\\\\n",
    "&= \\bigg( \\frac{1 - \\lambda}{\\lambda} \\bigg) \\bigg( \\frac{1}{\\epsilon^2} \\bigg) \\bigg(\n",
    "\\frac{1}{p} \\bigg).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Sanity checking this formula is easy. Here are the checks:\n",
    "\n",
    "- [ x ] The sample size $n$ is inversely related to our error tolerance, $\\epsilon$ or\n",
    "  $p$\n",
    "- [ x ] The sample size $n$ is inversely related to the prevalence of matching records\n",
    "  $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
