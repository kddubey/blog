{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**: custom Matryoshka + MNRL training implementation which might save\n",
    "memory. Need to evaluate by running this on GPU and monitoring memory.\n",
    "\n",
    "Modified\n",
    "https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/matryoshka/matryoshka_nli.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    "    losses,\n",
    "    util,\n",
    ")\n",
    "from sentence_transformers.evaluation import (\n",
    "    EmbeddingSimilarityEvaluator,\n",
    "    SequentialEvaluator,\n",
    "    SimilarityFunction,\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUSTOM = True\n",
    "FORCE_CPU = not torch.cuda.is_available()  # HF seems to always put stuff on MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the log level to INFO to get more information\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilroberta-base\"\n",
    "\n",
    "batch_size = 128 if not FORCE_CPU else 3\n",
    "num_batches = 20 if not FORCE_CPU else 3  #  limit training\n",
    "num_train_epochs = 1\n",
    "matryoshka_dims = [768, 512, 256, 128, 64]\n",
    "\n",
    "# Save path of the model\n",
    "output_dir = f\"output/matryoshka_nli_{model_name.replace('/', '-')}-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Here we define our SentenceTransformer model. If not already a Sentence Transformer model, it will automatically\n",
    "# create one with \"mean\" pooling.\n",
    "model = SentenceTransformer(model_name)\n",
    "# If we want, we can limit the maximum sequence length for the model\n",
    "# model.max_seq_length = 75\n",
    "logging.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FORCE_CPU:\n",
    "    model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the AllNLI dataset: https://huggingface.co/datasets/sentence-transformers/all-nli\n",
    "train_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"train\")\n",
    "logging.info(train_dataset)\n",
    "\n",
    "# If you wish, you can limit the number of training samples\n",
    "train_dataset = train_dataset.select(range(batch_size * num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define our training loss\n",
    "class MultipleNegativesRankingLoss(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SentenceTransformer,\n",
    "        scale: float = 20.0,\n",
    "        similarity_fct=util.cos_sim,\n",
    "    ) -> None:\n",
    "        super(MultipleNegativesRankingLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.scale = scale\n",
    "        self.similarity_fct = similarity_fct\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, embeddings_a: torch.Tensor, embeddings_b: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        embeddings_a = torch.nn.functional.normalize(embeddings_a, p=2, dim=-1)\n",
    "        embeddings_b = torch.nn.functional.normalize(embeddings_b, p=2, dim=-1)\n",
    "        scores: torch.Tensor = (\n",
    "            self.similarity_fct(embeddings_a, embeddings_b) * self.scale\n",
    "        )\n",
    "        print(scores.shape)\n",
    "        # Example a[i] should match with b[i]\n",
    "        range_labels = torch.arange(0, scores.size(0), device=scores.device)\n",
    "        return self.cross_entropy_loss(scores, range_labels)\n",
    "\n",
    "    def get_config_dict(self) -> dict[str, Any]:\n",
    "        return {\"scale\": self.scale, \"similarity_fct\": self.similarity_fct.__name__}\n",
    "\n",
    "    @property\n",
    "    def citation(self) -> str:\n",
    "        return \"\"\"\n",
    "@misc{henderson2017efficient,\n",
    "    title={Efficient Natural Language Response Suggestion for Smart Reply}, \n",
    "    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\n",
    "    year={2017},\n",
    "    eprint={1705.00652},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.CL}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatryoshkaTrainer(SentenceTransformerTrainer):\n",
    "    def __init__(self, matryoshka_dims: list[int], *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.matryoshka_dims = matryoshka_dims\n",
    "\n",
    "    def training_step(\n",
    "        self, model: torch.nn.Module, inputs: dict[str, torch.Tensor | Any]\n",
    "    ) -> torch.Tensor:\n",
    "        if FORCE_CPU:\n",
    "            # TODO: rm this stupid stuff Somewhere, the model gets moved to MPS,\n",
    "            # ignoring the use_mps_device=False flag. To get this mini-test working,\n",
    "            # force it to the CPU in the training step lol. I had to do this inside\n",
    "            # sentence_transformers.losses.MatryoshkaLoss.forward as well\n",
    "            model.to(\"cpu\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        features, labels = self.collect_features(inputs)\n",
    "\n",
    "        # TODO: need the bells and whistles from super class' training_step method.\n",
    "        # Should also be calling super's compute_loss instead\n",
    "\n",
    "        # Get full embedding matrix\n",
    "        reps = [\n",
    "            self.model(sentence_feature)[\"sentence_embedding\"]\n",
    "            for sentence_feature in features\n",
    "        ]\n",
    "        A_full: torch.Tensor = reps[0]\n",
    "        B_full = torch.cat(reps[1:])\n",
    "\n",
    "        # Detach it from the computation graph => don't back-propagate the gradient to\n",
    "        # the model yet\n",
    "        A_full_detached = A_full.detach()\n",
    "        A_full_detached.requires_grad = True\n",
    "        B_full_detached = B_full.detach()\n",
    "        B_full_detached.requires_grad = True\n",
    "\n",
    "        # From the super class' training_step method:\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Loop over dims, backwarding w/in each\n",
    "        tr_loss = torch.tensor(0.0)\n",
    "        for dim in self.matryoshka_dims:\n",
    "            loss: torch.Tensor = self.loss(\n",
    "                A_full_detached[..., :dim], B_full_detached[..., :dim], labels\n",
    "            )\n",
    "            # Accumulate the gradient for X_full_detached\n",
    "            self.accelerator.backward(loss)\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        # Apply chain rule to back-propagate the accumulated gradient to the model\n",
    "        A_full.backward(gradient=A_full_detached.grad)\n",
    "        B_full.backward(gradient=B_full_detached.grad)\n",
    "\n",
    "        # # Plain\n",
    "        # tr_loss = 0.0\n",
    "        # for dim in self.matryoshka_dims:\n",
    "        #     tr_loss += self.loss(A_full[..., :dim], B_full[..., :dim], labels)\n",
    "        # self.accelerator.backward(tr_loss)\n",
    "\n",
    "        return tr_loss.detach() / self.args.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define the training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=output_dir,\n",
    "    use_mps_device=False,\n",
    "    use_cpu=FORCE_CPU,\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=False,\n",
    "    bf16=not FORCE_CPU,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create the trainer\n",
    "if USE_CUSTOM:\n",
    "    print(\"Using **CUSTOM** Matryoshka + MNRL implementation\")\n",
    "    train_loss = MultipleNegativesRankingLoss(model)\n",
    "    trainer = MatryoshkaTrainer(\n",
    "        matryoshka_dims=matryoshka_dims,\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        loss=train_loss,\n",
    "    )\n",
    "else:\n",
    "    print(\"Using original Matryoshka + MNRL implementation\")\n",
    "    inner_train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "    train_loss = losses.MatryoshkaLoss(model, inner_train_loss, matryoshka_dims=matryoshka_dims)\n",
    "    trainer = SentenceTransformerTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        loss=train_loss,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluate the model performance on the STS Benchmark test dataset\n",
    "test_dataset = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
    "evaluators = []\n",
    "for dim in matryoshka_dims:\n",
    "    evaluators.append(\n",
    "        EmbeddingSimilarityEvaluator(\n",
    "            sentences1=test_dataset[\"sentence1\"],\n",
    "            sentences2=test_dataset[\"sentence2\"],\n",
    "            scores=test_dataset[\"score\"],\n",
    "            main_similarity=SimilarityFunction.COSINE,\n",
    "            name=f\"sts-test-{dim}\",\n",
    "            truncate_dim=dim,\n",
    "        )\n",
    "    )\n",
    "test_evaluator = SequentialEvaluator(evaluators)\n",
    "test_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save the trained & evaluated model locally\n",
    "final_output_dir = f\"{output_dir}/final\"\n",
    "model.save(final_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
