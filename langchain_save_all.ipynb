{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**: save all the stuff sent to the OpenAI API by LangChain as JSONs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import json\n",
    "import os\n",
    "from typing import Any, Callable\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy these functions somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could instead patch the class instead of the object. That way, any\n",
    "`langchain_openai.ChatOpenAI` object's `create` call will be saved as a JSON. For now,\n",
    "going to stick to the more conservative instance patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def _monkeypatch_instance_method(obj: Any, method_name: str, new_method: Callable):\n",
    "    original_method = getattr(obj, method_name)\n",
    "    # Need to use __get__ when patching instance methods\n",
    "    # https://stackoverflow.com/a/28127947/18758987\n",
    "    try:\n",
    "        setattr(obj, method_name, new_method.__get__(obj, obj.__class__))\n",
    "        yield\n",
    "    finally:\n",
    "        setattr(obj, method_name, original_method.__get__(obj, obj.__class__))\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def _save_method_inputs_as_json(obj: Any, method_name: str, jsons_dir: str):\n",
    "    # TODO: break if the method takes unnamed args. Use inspect.signature\n",
    "    original_method = getattr(obj, method_name)\n",
    "\n",
    "    @wraps(original_method)\n",
    "    def new_method(self, **kwargs):  # create takes only named args\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-%f\")\n",
    "        file_path = os.path.join(jsons_dir, f\"{current_time}.json\")\n",
    "        # TODO: consider logging the write\n",
    "        with open(file_path, mode=\"w\") as file:\n",
    "            json.dump(kwargs, file, indent=4)\n",
    "        return original_method(**kwargs)\n",
    "\n",
    "    with _monkeypatch_instance_method(obj, method_name, new_method):\n",
    "        yield\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def save_chat_create_inputs_as_jsons(client_with_create_method: Any, jsons_dir: str):\n",
    "    \"\"\"\n",
    "    In this context, save the inputs sent to some API through\n",
    "    `client_with_create_method.create` in `jsons_dir`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    client_with_create_method : Any\n",
    "        some object with a `create` method, e.g., `langchain_openai.ChatOpenAI()`. Its\n",
    "        inputs will be saved as JSONs whenver this method is called\n",
    "    jsons_dir : str\n",
    "        directory where your JSONs will get saved. The file names are timestamps\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ::\n",
    "\n",
    "        from langchain_openai import ChatOpenAI\n",
    "\n",
    "        llm = ChatOpenAI()\n",
    "        jsons_dir = \"temp\"\n",
    "\n",
    "        with save_chat_create_inputs_as_jsons(llm.client, jsons_dir):\n",
    "            # Note that the llm object is modified in this context, so any code that\n",
    "            # uses the llm will end up saving a JSON.\n",
    "            response = llm.invoke(\"how can langsmith help with testing?\")\n",
    "\n",
    "        # Then look at the json in ./jsons_dir\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    You probably only need the last JSON that's saved in `jsons_dir` b/c it'll contain\n",
    "    the chat history.\n",
    "    \"\"\"\n",
    "    with _save_method_inputs_as_json(client_with_create_method, \"create\", jsons_dir):\n",
    "        yield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with save_chat_create_inputs_as_jsons(llm.client, jsons_dir=\"temp\"):\n",
    "    response = llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langsmith can help with testing in several ways:\\n\\n1. Automated testing: Langsmith can be used to write scripts for automated testing of software applications. This can help to ensure that the software functions correctly and meets the specified requirements.\\n\\n2. Test case generation: Langsmith can be used to generate test cases automatically based on the requirements of the software application. This can help to improve test coverage and ensure that all possible scenarios are tested.\\n\\n3. Test data generation: Langsmith can be used to generate test data automatically for testing purposes. This can help to ensure that the software application behaves correctly with different types of input data.\\n\\n4. Test reporting: Langsmith can be used to generate test reports automatically, providing detailed information on the results of the testing process. This can help to identify any issues or defects in the software application.\\n\\nOverall, Langsmith can help to streamline the testing process, improve test coverage, and ensure the quality of software applications.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo w/ CoT and a tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few cells are just setup. See the last few cells in the notebook.\n",
    "\n",
    "Demo is from\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/07-langchain-tools.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from math import pi\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"Circumference calculator\"\n",
    "    description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        if isinstance(radius, str):\n",
    "            # stupid thing. I checked that this happens in the raw demo\n",
    "            radius = radius.lstrip(\"radius=: \")\n",
    "        return float(radius) * 2.0 * pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\"\n",
    "\n",
    "# initialize LLM (we use ChatOpenAI because we'll later define a `chat` agent)\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY, temperature=0, model_name=\"gpt-3.5-turbo\"\n",
    ")\n",
    "# initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdubey/Envs/fred/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "tools = [CircumferenceTool()]\n",
    "\n",
    "# initialize agent with tools\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=conversational_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kdubey/Envs/fred/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Circumference calculator\",\n",
      "    \"action_input\": \"radius=2\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m12.566370614359172\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The circumference of the circle with a radius of 2 is approximately 12.57 units.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jsons_dir = \"temp\"  # make this on your own\n",
    "with save_chat_create_inputs_as_jsons(llm.client, jsons_dir):\n",
    "    agent(\n",
    "        \"I have a circle with diameter 4. First calculate its radius. \"\n",
    "        \"That'll be your first observation. \"\n",
    "        \"In a new observation, calculate its circumference.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the last JSON looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"messages\": [\n",
      "        {\n",
      "            \"role\": \"system\",\n",
      "            \"content\": \"Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\"\n",
      "        },\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": \"TOOLS\\n------\\nAssistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\\n\\n> Circumference calculator: use this tool when you need to calculate a circumference using the radius of a circle\\n\\nRESPONSE FORMAT INSTRUCTIONS\\n----------------------------\\n\\nWhen responding to me, please output a response in one of two formats:\\n\\n**Option 1:**\\nUse this if you want the human to use a tool.\\nMarkdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": string, \\\\\\\\ The action to take. Must be one of Circumference calculator\\n    \\\"action_input\\\": string \\\\\\\\ The input to the action\\n}\\n```\\n\\n**Option #2:**\\nUse this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\\n\\n```json\\n{\\n    \\\"action\\\": \\\"Final Answer\\\",\\n    \\\"action_input\\\": string \\\\\\\\ You should put what you want to return to use here\\n}\\n```\\n\\nUSER'S INPUT\\n--------------------\\nHere is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\\n\\nI have a circle with diameter 4. First calculate its radius. That'll be your first observation. In a new observation, calculate its circumference.\"\n",
      "        },\n",
      "        {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"```json\\n{\\n    \\\"action\\\": \\\"Circumference calculator\\\",\\n    \\\"action_input\\\": \\\"radius=2\\\"\\n}\\n```\"\n",
      "        },\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": \"TOOL RESPONSE: \\n---------------------\\n12.566370614359172\\n\\nUSER'S INPUT\\n--------------------\\n\\nOkay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else.\"\n",
      "        }\n",
      "    ],\n",
      "    \"model\": \"gpt-3.5-turbo\",\n",
      "    \"stream\": false,\n",
      "    \"n\": 1,\n",
      "    \"temperature\": 0.0,\n",
      "    \"stop\": [\n",
      "        \"\\nObservation:\",\n",
      "        \"\\n\\tObservation:\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "_json_files = os.listdir(jsons_dir)\n",
    "_num_jsons = len(_json_files)\n",
    "\n",
    "last_json_file = sorted(_json_files)[-1]\n",
    "with open(os.path.join(jsons_dir, last_json_file), \"r\") as f:\n",
    "    last_inputs = json.load(f)\n",
    "print(json.dumps(last_inputs, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the context manager isn't used after it was used, verify that no extra JSONs are\n",
    "saved, i.e., we're back to using the old `create` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Circumference calculator\",\n",
      "    \"action_input\": \"radius=2\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m12.566370614359172\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The circumference of the circle with a radius of 2 is approximately 12.57 units.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"I have a circle with diameter 4. First calculate its radius. That'll be your first observation. In a new observation, calculate its circumference.\",\n",
       " 'chat_history': [HumanMessage(content=\"I have a circle with diameter 4. First calculate its radius. That'll be your first observation. In a new observation, calculate its circumference.\"),\n",
       "  AIMessage(content='The circumference of the circle with a radius of 2 is approximately 12.57 units.')],\n",
       " 'output': 'The circumference of the circle with a radius of 2 is approximately 12.57 units.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\n",
    "    \"I have a circle with diameter 4. First calculate its radius. \"\n",
    "    \"That'll be your first observation. \"\n",
    "    \"In a new observation, calculate its circumference.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(os.listdir(jsons_dir)) == _num_jsons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
